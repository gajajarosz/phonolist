ID: lingbuzz__lingbuzz_002260
URL: http://ling.auf.net/lingbuzz/002260
PDFURL: http://ling.auf.net/lingbuzz/002260/current.pdf?_s=rQMXfVfs-DSMTtzP
HEADER: Rasin & Katzir (2017) - A learnability argument for constraints on underlying representations
TITLE: A learnability argument for constraints on underlying representations
AUTHORTITLE: Rasin & Katzir (2017)
RAWAUTHORS: <a href="http://ling.auf.net/lingbuzz/002260?_s=ToxFIwmbZq0tAoLT&amp;_k=pH9c1eQnnip6SlVd&amp;1">Ezer Rasin</a>, <a href="http://ling.auf.net/lingbuzz/002260?_s=ToxFIwmbZq0tAoLT&amp;_k=pH9c1eQnnip6SlVd&amp;2">Roni Katzir</a>
AUTHORS: Ezer Rasin, Roni Katzir
LASTNAMES: Rasin, Katzir
MONTH: September
YEAR: 2017
ABSTRACT: Speakers judge some nonce forms as nonexistent but possible – that is, as accidental gaps – and other nonce forms as nonexistent and impossible – that is, as systematic gaps. Early generative approaches accounted for systematic gaps through a combination of two factors: constraints on underlying representations in the lexicon; and phonological rules. Contrasting with this view, Optimality Theory has been guided by the idea that phonological generalizations are captured not in the lexicon but rather on the surface or in the mapping from URs to surface forms. The view that there are no constraints on URs is often referred to as Richness of the Base (ROTB), and it is a central tenet of OT.    Our goal in this note is to re-open the question of whether OT requires constraints on URs and offer a learnability argument supporting an affirmative answer, thus arguing against ROTB. We start by examining the extant literature on learning in OT and argue that the learners proposed there overgeneralize (by treating some systematic gaps as accidental), undergeneralize (by treating some accidental gaps as systematic), or both. We then discuss a different approach to learning, compression-based learning, that is the only approach currently available that can handle the data in principle without over- or undergeneralization. We show that compression-based learning learns certain naturally-occurring patterns, but crucially only if it rejects ROTB and employs language-specific constraints on URs.
HTML: <font size="+1"><b><a href="http://ling.auf.net/lingbuzz/002260/current.pdf?_s=rQMXfVfs-DSMTtzP">A learnability argument for constraints on underlying representations</a></b></font><br/><a href="http://ling.auf.net/lingbuzz/002260?_s=ToxFIwmbZq0tAoLT&amp;_k=pH9c1eQnnip6SlVd&amp;1">Ezer Rasin</a>, <a href="http://ling.auf.net/lingbuzz/002260?_s=ToxFIwmbZq0tAoLT&amp;_k=pH9c1eQnnip6SlVd&amp;2">Roni Katzir</a><br/>September 2017</center>&nbsp;<p></p>Speakers judge some nonce forms as nonexistent but possible – that is, as accidental gaps – and other nonce forms as nonexistent and impossible – that is, as systematic gaps. Early generative approaches accounted for systematic gaps through a combination of two factors: constraints on underlying representations in the lexicon; and phonological rules. Contrasting with this view, Optimality Theory has been guided by the idea that phonological generalizations are captured not in the lexicon but rather on the surface or in the mapping from URs to surface forms. The view that there are no constraints on URs is often referred to as Richness of the Base (ROTB), and it is a central tenet of OT.    Our goal in this note is to re-open the question of whether OT requires constraints on URs and offer a learnability argument supporting an affirmative answer, thus arguing against ROTB. We start by examining the extant literature on learning in OT and argue that the learners proposed there overgeneralize (by treating some systematic gaps as accidental), undergeneralize (by treating some accidental gaps as systematic), or both. We then discuss a different approach to learning, compression-based learning, that is the only approach currently available that can handle the data in principle without over- or undergeneralization. We show that compression-based learning learns certain naturally-occurring patterns, but crucially only if it rejects ROTB and employs language-specific constraints on URs.<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="http://ling.auf.net/lingbuzz/002260/current.pdf?_s=aMTIY5bR5sg1pMml">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/002260<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>Submitted</td></tr><tr><td>keywords: </td><td>learning, evaluation metrics, minimum description length, optimality theory, morpheme structure constraints, richness of the base, phonology</td></tr><tr><td>previous versions: </td><td><a href="http://ling.auf.net/lingbuzz/002260/v1.pdf?_s=ATDpuHk37h2WSNpM">v1 [October 2014]</a><br/></td></tr>
