ID: lingbuzz__lingbuzz_003367
URL: http://ling.auf.net/lingbuzz/003367
PDFURL: "/lingbuzz/003367/current.pdf?_s=vO-kHfflM2uR7qTq"
HEADER: Cotterell, Peng & Eisner (2015) - Modeling Word Forms Using Latent Underlying Morphs and Phonology
TITLE: Modeling Word Forms Using Latent Underlying Morphs and Phonology
AUTHORTITLE: Cotterell, Peng & Eisner (2015)
RAWAUTHORS: <a href="/lingbuzz/003367?_s=38Ue8X27qS8ii_ot&amp;_k=q1rpnvjmJd7FhBqe&amp;1">Ryan Cotterell</a>, <a href="/lingbuzz/003367?_s=38Ue8X27qS8ii_ot&amp;_k=q1rpnvjmJd7FhBqe&amp;2">Nanyun Peng</a>, <a href="/lingbuzz/003367?_s=38Ue8X27qS8ii_ot&amp;_k=q1rpnvjmJd7FhBqe&amp;3">Jason Eisner</a>
AUTHORS: Ryan Cotterell, Nanyun Peng, Jason Eisner
LASTNAMES: Cotterell, Peng, Eisner
MONTH: August
YEAR: 2015
ABSTRACT: The observed pronunciations or spellings of words are often explained as arising from the “underlying forms” of their morphemes. These forms are latent strings that linguists try to reconstruct by hand. We propose to reconstruct them automatically at scale, enabling generalization to new words. Given some surface word types of a concatenative language along with the abstract morpheme sequences that they express, we show how to recover consistent underlying forms for these morphemes, together with the (stochastic) phonology that maps each concatenation of underlying forms to a surface form. Our technique involves loopy belief propagation in a natural directed graphical model whose variables are unknown strings and whose conditional distributions are encoded as finitestate machines with trainable weights. We define training and evaluation paradigms for the task of surface word prediction, and report results on subsets of 7 languages
HTML: <font size="+1"><b><a href="/lingbuzz/003367/current.pdf?_s=vO-kHfflM2uR7qTq">Modeling Word Forms Using Latent Underlying Morphs and Phonology</a></b></font><br/><a href="/lingbuzz/003367?_s=38Ue8X27qS8ii_ot&amp;_k=q1rpnvjmJd7FhBqe&amp;1">Ryan Cotterell</a>, <a href="/lingbuzz/003367?_s=38Ue8X27qS8ii_ot&amp;_k=q1rpnvjmJd7FhBqe&amp;2">Nanyun Peng</a>, <a href="/lingbuzz/003367?_s=38Ue8X27qS8ii_ot&amp;_k=q1rpnvjmJd7FhBqe&amp;3">Jason Eisner</a><br/>August 2015</center>&nbsp;<p></p>The observed pronunciations or spellings of words are often explained as arising from the “underlying forms” of their morphemes. These forms are latent strings that linguists try to reconstruct by hand. We propose to reconstruct them automatically at scale, enabling generalization to new words. Given some surface word types of a concatenative language along with the abstract morpheme sequences that they express, we show how to recover consistent underlying forms for these morphemes, together with the (stochastic) phonology that maps each concatenation of underlying forms to a surface form. Our technique involves loopy belief propagation in a natural directed graphical model whose variables are unknown strings and whose conditional distributions are encoded as finitestate machines with trainable weights. We define training and evaluation paradigms for the task of surface word prediction, and report results on subsets of 7 languages<table cellspacing="15" valign="top"><tr><td>Format: </td><td>[ <a href="/lingbuzz/003367/current.pdf?_s=SCOvkJKoLcqgXYWZ">pdf</a> ]</td></tr><tr><td>Reference: </td><td>lingbuzz/003367<br/><font size="-1"> (please use that when you cite this article)</font></td></tr><tr><td>Published in: </td><td>TACL</td></tr><tr><td>keywords: </td><td>computational phonology, morphology, phonology</td></tr>
